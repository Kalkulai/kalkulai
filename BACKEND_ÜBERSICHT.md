# Kalkulai Backend - Technische Ãœbersicht

## ğŸ¯ Zweck des Systems

Kalkulai ist ein intelligentes Angebotserstellungssystem fÃ¼r Handwerker (speziell Maler- und Lackiererbetriebe). Das Backend verarbeitet natÃ¼rliche Sprache, erkennt Materialbedarf, generiert Angebote und erstellt PDF-Dokumente.

---

## ğŸ—ï¸ Architektur-Ãœbersicht

```
Frontend (React/Vite)
    â†“ HTTP/REST
FastAPI Backend (main.py)
    â†“
Service Layer (quote_service.py) â† Single Source of Truth
    â†“
â”œâ”€â”€ LLM Integration (llm.py)
â”œâ”€â”€ Vector Search (retriever/)
â”œâ”€â”€ Database Layer (store/)
â”œâ”€â”€ PDF Generation (pdf.py)
â””â”€â”€ MCP Server (mcp/) - Optional fÃ¼r externe LLM-Hosts
```

### Kernprinzipien

1. **Service Layer First**: Alle Business-Logik liegt in `quote_service.py`
2. **Zwei-LLM-Architektur**: LLM1 (Chat/Erfassung) + LLM2 (Angebotsgenerierung)
3. **Hybrid Search**: BM25 + Lexical + RRF fÃ¼r Produktsuche
4. **Multi-Tenant**: Company-basierte Datenisolation
5. **Stateful Sessions**: Wizard-Sessions fÃ¼r mehrstufige Workflows

---

## ğŸ“ Projektstruktur

```
backend/
â”œâ”€â”€ main.py                    # FastAPI App Entry Point
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ quote_service.py   # â­ KERN-MODUL: Alle Business-Logik
â”‚   â”œâ”€â”€ llm.py                 # LLM-Initialisierung & Chain-Building
â”‚   â”œâ”€â”€ db.py                  # Legacy: Statische Produktdateien laden
â”‚   â”œâ”€â”€ pdf.py                 # PDF-Generierung (WeasyPrint + Jinja2)
â”‚   â”œâ”€â”€ admin_api.py           # CRUD fÃ¼r Produkte (REST API)
â”‚   â”œâ”€â”€ auth_api.py            # Authentifizierung (JWT)
â”‚   â”œâ”€â”€ auth.py                # User-Management (SQLite)
â”‚   â”œâ”€â”€ offers_api.py          # Angebots-Verwaltung
â”‚   â”œâ”€â”€ speech_api.py           # Azure Speech-to-Text
â”‚   â”œâ”€â”€ uom_convert.py         # Einheiten-Umrechnung
â”‚   â”œâ”€â”€ utils.py               # Helper-Funktionen
â”‚   â””â”€â”€ mcp/                   # Model Context Protocol Server
â”‚       â”œâ”€â”€ server.py          # MCP JSON-over-stdio Dispatcher
â”‚       â””â”€â”€ tools.py           # MCP Tool Wrapper
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ thin.py                # â­ Hybrid Search (BM25 + Lexical + RRF)
â”‚   â”œâ”€â”€ index_manager.py        # Vector Index Management (DocArray)
â”‚   â”œâ”€â”€ hybrid_search.py       # BM25 Implementation
â”‚   â””â”€â”€ main.py                # Legacy: ChromaDB Retriever
â”œâ”€â”€ store/
â”‚   â””â”€â”€ catalog_store.py       # â­ SQLModel ORM fÃ¼r Produkte
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ normalize/              # Text-Normalisierung & Synonyme
â”‚   â””â”€â”€ package_converter.py   # Gebinde-Umrechnung
â”œâ”€â”€ templates/                  # Jinja2 PDF-Templates
â”œâ”€â”€ data/                      # Statische Produktdateien (.txt)
â””â”€â”€ var/                       # SQLite DB (kalkulai.db)
```

---

## ğŸ”‘ Kernkomponenten im Detail

### 1. Service Layer (`app/services/quote_service.py`)

**Rolle**: Single Source of Truth fÃ¼r alle Business-Logik

**Wichtigste Funktionen**:
- `chat_turn()` - LLM1 Chat-Interaktion (Erfassung von Projektanforderungen)
- `generate_offer_positions()` - LLM2 Angebotsgenerierung (JSON-Output)
- `search_catalog()` - Produktsuche mit Fuzzy-Matching
- `render_offer_or_invoice_pdf()` - PDF-Generierung
- `wizard_next_step()` / `wizard_finalize()` - Wizard-Workflow
- `run_revenue_guard()` - MargenprÃ¼fung

**QuoteServiceContext**:
- Zentraler Context-Container mit:
  - LLM-Instanzen (llm1, llm2)
  - LangChain Chains (chain1, chain2)
  - Memory (memory1)
  - Retriever
  - Catalog-Datenstrukturen
  - Wizard-Sessions
  - Jinja2 Environment
  - Konfiguration (VAT, Thresholds, etc.)

**Design-Pattern**: 
- Alle Funktionen nehmen `ctx: QuoteServiceContext` als Parameter
- Keine globalen ZustÃ¤nde innerhalb des Service-Layers
- ServiceError fÃ¼r strukturierte Fehlerbehandlung

---

### 2. LLM-Integration (`app/llm.py`)

**Zwei-LLM-System**:

**LLM1** (Chat/Erfassung):
- **Modell**: `MODEL_LLM1` (default: `gpt-4o-mini`)
- **Temperature**: 0.15 (kreativer, aber kontrolliert)
- **Aufgabe**: 
  - Projektanforderungen erfassen
  - Materialbedarf schÃ¤tzen
  - RÃ¼ckfragen stellen
  - Status-Tracking (noch_zu_leisten / bereits_erledigt / unklar)
- **Memory**: ConversationBufferWindowMemory (letzte N Nachrichten)
- **Output**: Markdown-Text + Maschinenanhang (projekt_id, version, status, materialien[])

**LLM2** (Angebotsgenerierung):
- **Modell**: `MODEL_LLM2` (default: `gpt-4o-mini`)
- **Temperature**: 0.0 (deterministisch)
- **Aufgabe**: 
  - Strukturierte Angebotspositionen generieren (JSON)
  - Preise berechnen
  - Einheiten harmonisieren
- **Input**: Chat-History + Catalog-Candidates
- **Output**: JSON mit positions[], raw_llm, error?

**Chain-Building**:
- `build_chains()` erstellt LangChain Chains mit Retrieval-Augmented Generation (RAG)
- Retriever wird in Chain integriert fÃ¼r Kontext-Erweiterung
- Prompts sind in `llm.py` definiert (sehr detailliert, ~200 Zeilen)

**Provider-Support**:
- OpenAI (default)
- Ollama (lokal)
- HuggingFace (optional)

---

### 3. Produktsuche - Zwei Retriever-Systeme

Das System verwendet **zwei verschiedene Retriever**, die je nach Use-Case eingesetzt werden:

---

#### **A) Hybrid Search (`retriever/thin.py`)** âš¡

**Schnelle, deterministische Produktsuche ohne LLM.**

**Technologie**:
1. **BM25** (Keyword-basiert):
   - Term-Frequency / Inverse Document Frequency
   - Gute Performance fÃ¼r exakte Matches

2. **Lexical Search** (Fuzzy-Matching):
   - SequenceMatcher fÃ¼r Ã„hnlichkeit
   - Synonym-Erkennung
   - Prefix/Substring-Boosts
   - Confusion-Pair-Penalties (z.B. "krepp" vs "kreide")

3. **RRF** (Reciprocal Rank Fusion):
   - Kombiniert BM25 + Lexical Scores
   - Formel: `score = 1 / (k + rank)` fÃ¼r beide Rankings

**Scoring-Boni**:
- Exact Match: +0.25
- Synonym Hit: +0.15
- Keyword Match: +0.15
- Price Available: +0.03
- Confusion Penalty: -0.3

**Pre-Filtering**:
- Test-Produkte werden ausgefiltert (SKU-Prefixes: `test-`, `demo-`)
- Inactive Products werden ignoriert
- Category-Filterung mÃ¶glich

**Wann wird Hybrid Search verwendet?**

1. **LLM1 Chat (`chat_turn()`)**:
   - **Nur wenn** `LLM1_THIN_RETRIEVAL=1` gesetzt ist
   - Funktion: `_build_catalog_candidates()` â†’ `_run_thin_catalog_search()`
   - Zweck: Schnelle ProduktvorschlÃ¤ge wÃ¤hrend des Chats
   - Output: Katalog-VorschlÃ¤ge werden in Chat-Antwort eingefÃ¼gt

2. **API `/api/catalog/search`**:
   - **PrimÃ¤r** wenn kein Vector Retriever vorhanden (`ctx.retriever == None`)
   - Funktion: `search_catalog()` â†’ `_catalog_lookup()` â†’ Fallback zu `search_catalog_thin()`
   - Zweck: Direkte Produktsuche via API

3. **Material-Validierung**:
   - Funktion: `_validate_materials()` â†’ `_run_thin_catalog_search()`
   - Zweck: PrÃ¼fung ob Materialien im Katalog existieren

**Vorteile**:
- âœ… Sehr schnell (keine Embedding-Generierung)
- âœ… Deterministisch (reproduzierbare Ergebnisse)
- âœ… Gute Tippfehlerkorrektur
- âœ… Synonym-Erkennung
- âœ… Keine LLM-AbhÃ¤ngigkeit

---

#### **B) Vector Index (`retriever/index_manager.py`)** ğŸ§ 

**Semantische Suche Ã¼ber Embeddings.**

**Technologie**:
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
- **Backend**: DocArray InMemoryExactNN
- **Similarity**: Cosine Similarity
- **Multi-Tenant**: Separate Indizes pro Company-ID

**Wann wird Vector Index verwendet?**

1. **LLM1 Chat Context-Erweiterung**:
   - **Wenn** `ctx.retriever` vorhanden ist UND `LLM1_THIN_RETRIEVAL=0`
   - Funktion: `chat_turn()` â†’ LangChain Chain mit Retriever
   - Zweck: Semantische Kontext-Erweiterung fÃ¼r LLM1
   - Input: Chat-History wird als Query verwendet

2. **LLM2 Angebotsgenerierung (`generate_offer_positions()`)**:
   - **Immer** wenn `ctx.retriever` vorhanden ist
   - Funktion: `find_exact_catalog_lines()` â†’ `ctx.retriever.get_relevant_documents()`
   - Zweck: Semantische Suche fÃ¼r Produktkontext
   - Verwendung: Context-Erweiterung fÃ¼r LLM2 Prompt

3. **Ranking (`rank_main()`)**:
   - Funktion: `_run_rank_main()` â†’ `rank_main()` â†’ `ctx.retriever.get_relevant_documents()`
   - Zweck: Business-Scoring mit semantischen Kandidaten
   - Verwendung: In `generate_offer_positions()` fÃ¼r bessere Produktauswahl

4. **Company-spezifische Suche**:
   - Funktion: `_company_catalog_search()` â†’ `index_manager.search_index()`
   - Zweck: Multi-Tenant Produktsuche

**Vorteile**:
- âœ… Semantisches VerstÃ¤ndnis ("AuÃŸenfarbe" = "Fassadenfarbe")
- âœ… BedeutungsÃ¤hnlichkeit (nicht nur Keyword-Match)
- âœ… Gute Performance bei unklaren Begriffen
- âœ… Multi-Tenant Support

**Nachteile**:
- âš ï¸ Langsamer als Hybrid Search (Embedding-Generierung)
- âš ï¸ BenÃ¶tigt initialen Index-Build
- âš ï¸ AbhÃ¤ngig von Embedding-Modell-QualitÃ¤t

---

#### **Entscheidungslogik: Welcher Retriever wird wann verwendet?**

```python
# 1. LLM1 Chat - Katalog-VorschlÃ¤ge
if ctx.llm1_thin_retrieval == True:
    # â†’ Hybrid Search (thin.py)
    candidates = _build_catalog_candidates()  # Nutzt search_catalog_thin()
else:
    # â†’ Vector Index (wenn retriever vorhanden)
    # â†’ LangChain Chain mit Retriever fÃ¼r Context-Erweiterung

# 2. API /api/catalog/search
if ctx.retriever is None:
    # â†’ Hybrid Search (thin.py) als Fallback
    results = search_catalog_thin()
else:
    # â†’ Vector Index (index_manager)
    docs = ctx.retriever.get_relevant_documents(query)

# 3. LLM2 Angebotsgenerierung
if ctx.retriever is not None:
    # â†’ Vector Index fÃ¼r semantische Context-Erweiterung
    ctx_lines = find_exact_catalog_lines()  # Nutzt retriever
    ranked = rank_main()  # Nutzt retriever fÃ¼r Business-Scoring
```

**Konfiguration**:
- `LLM1_THIN_RETRIEVAL=1` â†’ Hybrid Search fÃ¼r LLM1 Chat-VorschlÃ¤ge
- `LLM1_THIN_RETRIEVAL=0` â†’ Vector Index fÃ¼r LLM1 Context (wenn verfÃ¼gbar)
- Vector Index wird **immer** fÃ¼r LLM2 verwendet (wenn verfÃ¼gbar)

**Empfehlung**:
- **Hybrid Search**: FÃ¼r schnelle, deterministische Suchen, Tippfehlerkorrektur
- **Vector Index**: FÃ¼r semantische Suche, unklare Begriffe, Context-Erweiterung
- **Kombination**: Beide kÃ¶nnen parallel verwendet werden fÃ¼r verschiedene Use-Cases

**Technische Details Vector Index**:
- **Embedding-Modell**: `sentence-transformers/all-MiniLM-L6-v2` (default)
- **Backend**: DocArray InMemoryExactNN (Cosine Similarity)
- **Company-scoped**: Jede Company hat eigenen Index
- **Auto-Rebuild**: Bei Produkt-Updates wird Index neu gebaut
- **Thread-Safety**: Lock-Mechanismus fÃ¼r Cache-Updates
- **Fallback**: Wenn DocArray nicht verfÃ¼gbar â†’ einfache Cosine-Similarity Implementation

---

#### **Zusammenfassung: Retriever-Verwendung**

| Use-Case | Retriever | Bedingung | Funktion |
|----------|-----------|-----------|----------|
| **LLM1 Chat - Katalog-VorschlÃ¤ge** | Hybrid Search | `LLM1_THIN_RETRIEVAL=1` | `_build_catalog_candidates()` |
| **LLM1 Chat - Standard** | Kein Retriever | `LLM1_THIN_RETRIEVAL=0` (default) | `chain1.run()` (nur Memory) |
| **LLM2 Angebotsgenerierung** | Vector Index | `retriever` vorhanden | `find_exact_catalog_lines()` + `rank_main()` |
| **API `/api/catalog/search`** | Hybrid Search | `retriever == None` (Fallback) | `search_catalog()` |
| **API `/api/catalog/search`** | Hybrid + Vector | `COMBINE_HYBRID_VECTOR=1` + `retriever` vorhanden | `search_catalog()` |
| **Material-Validierung** | Hybrid Search | Immer | `_validate_materials()` |
| **Business-Scoring** | Vector Index | `retriever` vorhanden | `rank_main()` |
| **Company-Suche** | Vector Index | Immer | `_company_catalog_search()` |

**Wichtig: LLM1 vs LLM2**:
- **LLM1**: 
  - `LLM1_THIN_RETRIEVAL=0` (default) â†’ Kein Retriever, nur Memory
  - `LLM1_THIN_RETRIEVAL=1` â†’ Hybrid Search fÃ¼r Katalog-VorschlÃ¤ge (wird in Antwort eingefÃ¼gt)
  - Chain1 hat **keinen Retriever integriert** (nur LLMChain)
- **LLM2**: 
  - Vector Index wird **immer** verwendet (wenn `retriever` vorhanden)
  - Chain2 nutzt `ConversationalRetrievalChain` mit Vector Retriever
  - FÃ¼r Context-Erweiterung und Business-Scoring

---

### 5. Datenbank-Layer (`store/catalog_store.py`)

**SQLModel ORM** (SQLAlchemy-basiert):

**Tabellen**:

**products**:
```python
- id (PK)
- company_id (Index)  # Multi-Tenant
- sku (Index, Unique mit company_id)
- name, description
- price_eur, unit, volume_l
- category, material_type, unit_package, tags
- is_active (Index)
- updated_at (Index)
```

**synonyms**:
```python
- id (PK)
- company_id (Index)
- canon, variant (Unique mit company_id)
- confidence
- updated_at
```

**Wichtigste Funktionen**:
- `create_product()` / `update_product()` / `delete_product()`
- `get_active_products(company_id)` - Filtert inactive + test products
- `upsert_synonym()` - Synonym-Verwaltung
- `init_db()` - Schema-Erstellung

**DB-URL**: 
- Default: `sqlite:///backend/var/kalkulai.db`
- Konfigurierbar via `DB_URL` oder `KALKULAI_DB_URL`

**Migration**: 
- SQLModel erstellt Tabellen automatisch beim ersten Start

---

### 6. PDF-Generierung (`app/pdf.py`)

**WeasyPrint + Jinja2**:

**Templates**:
- `offer.html` - Standard-Template
- `offer_modern.html` - Modernes Design
- `offer_premium.html` - Premium-Variante
- `offer_custom.html` - Customizable

**Workflow**:
1. Jinja2 Template laden
2. Context-Daten einfÃ¼gen (positions[], customer_info, etc.)
3. WeasyPrint rendert HTML â†’ PDF
4. PDF wird in `OUTPUT_DIR` gespeichert
5. URL wird zurÃ¼ckgegeben (`/outputs/{filename}.pdf`)

**Features**:
- Currency-Formatierung (Jinja2 Filter)
- Date-Formatierung
- Responsive Layout
- Logo-Integration

**Static Files**:
- FastAPI mountet `/outputs` fÃ¼r PDF-Downloads

---

### 7. API-Endpoints (`main.py`)

**Haupt-Endpoints**:

```
POST /api/chat              # LLM1 Chat-Turn
POST /api/offer             # LLM2 Angebotsgenerierung
POST /api/pdf               # PDF-Rendering
POST /api/session/reset     # Memory & Wizard Reset

GET  /api/catalog/search    # Produktsuche
GET  /api/catalog           # Catalog-Ãœbersicht

POST /wizard/maler/next     # Wizard-Step
POST /wizard/maler/finalize # Wizard-Abschluss

POST /revenue-guard/check   # MargenprÃ¼fung
```

**Admin-Endpoints** (`admin_api.py`):
```
POST   /api/admin/products        # Produkt erstellen
PUT    /api/admin/products/{sku}   # Produkt aktualisieren
DELETE /api/admin/products/{sku}   # Produkt lÃ¶schen
POST   /api/admin/products/rebuild # Index neu bauen
```

**Auth-Endpoints** (`auth_api.py`):
```
POST /api/auth/login        # JWT-Token generieren
POST /api/auth/register     # User-Registrierung
GET  /api/auth/me           # Current User
```

**CORS**:
- Konfigurierbar via `FRONTEND_ORIGINS`
- Default: `localhost:5173`, HuggingFace Spaces

---

### 8. MCP Server (`app/mcp/`)

**Model Context Protocol** - Externe LLM-Host-Integration:

**Architektur**:
```
LLM Host (Claude Desktop, etc.)
    â†“ JSON/stdio
MCP Server (server.py)
    â†“
MCP Tools (tools.py)
    â†“
Quote Service Layer
```

**Tools**:
- `reset_session` - Session zurÃ¼cksetzen
- `chat_turn` - Chat-Interaktion
- `generate_offer_positions` - Angebot generieren
- `render_pdf` - PDF erstellen
- `wizard_next_step` - Wizard-Step
- `revenue_guard_check` - MargenprÃ¼fung

**Vorteil**: 
- Externe LLM-Hosts kÃ¶nnen Kalkulai-Funktionen direkt aufrufen
- Keine neuen HTTP-Endpoints nÃ¶tig
- Type-Safe Tool-Definitionen

---

## ğŸ”„ Datenfluss-Beispiele

### Beispiel 1: Chat â†’ Angebot â†’ PDF

```
1. User: "Ich brauche Angebot fÃ¼r 50mÂ² Wand streichen"
   â†“
2. POST /api/chat
   â†’ quote_service.chat_turn()
   â†’ LLM1 verarbeitet Anfrage
   â†’ RÃ¼ckfragen oder Materialliste
   â†“
3. User: "Passt so"
   â†“
4. POST /api/offer
   â†’ quote_service.generate_offer_positions()
   â†’ LLM2 generiert JSON mit Positionen
   â†’ Catalog-Suche fÃ¼r Preise
   â†’ Einheiten-Harmonisierung
   â†“
5. POST /api/pdf
   â†’ quote_service.render_offer_or_invoice_pdf()
   â†’ Jinja2 Template + WeasyPrint
   â†’ PDF gespeichert
   â†’ URL zurÃ¼ckgegeben
```

### Beispiel 2: Produktsuche

```
1. User sucht "Dispersionsfarbe weiÃŸ"
   â†“
2. POST /api/catalog/search?q=dispersionsfarbe+weiÃŸ
   â†“
3. quote_service.search_catalog()
   â†’ retriever.thin.search_catalog_thin()
   â†’ Hybrid Search:
      - BM25: Keyword-Match
      - Lexical: Fuzzy-Match + Synonyme
      - RRF: Score-Fusion
   â†’ Top-K Ergebnisse zurÃ¼ckgeben
```

### Beispiel 3: Produkt erstellen

```
1. POST /api/admin/products
   â†’ admin_api.create_product()
   â†’ catalog_store.create_product()
   â†’ DB INSERT
   â†’ index_manager.invalidate_index()
   â†’ refresh_catalog_cache()
   â†’ Index wird beim nÃ¤chsten Search neu gebaut
```

---

## âš™ï¸ Konfiguration (Environment Variables)

**LLM**:
- `MODEL_PROVIDER` - `openai` | `ollama` | `huggingface`
- `MODEL_LLM1` - Modell fÃ¼r Chat (default: `gpt-4o-mini`)
- `MODEL_LLM2` - Modell fÃ¼r Angebot (default: `gpt-4o-mini`)
- `OPENAI_API_KEY` - API-Key fÃ¼r OpenAI
- `OLLAMA_BASE_URL` - URL fÃ¼r lokalen Ollama-Server

**Datenbank**:
- `DB_URL` / `KALKULAI_DB_URL` - Datenbank-URL
- `DATA_ROOT` - Root-Verzeichnis fÃ¼r Daten
- `CHROMA_DIR` - Verzeichnis fÃ¼r ChromaDB (Legacy)

**Search**:
- `CATALOG_TOP_K` - Anzahl Ergebnisse (default: 5)
- `CATALOG_CACHE_TTL` - Cache-TTL in Sekunden (default: 60)
- `CATALOG_QUERIES_PER_TURN` - Max. Suchqueries pro Chat-Turn (default: 2)
- `LLM1_THIN_RETRIEVAL` - **Wichtig**: 
  - `0` (default) â†’ Vector Index fÃ¼r LLM1 Context-Erweiterung
  - `1` â†’ Hybrid Search fÃ¼r LLM1 Katalog-VorschlÃ¤ge (schneller, deterministisch)
- `COMBINE_HYBRID_VECTOR` - **Neu**: Kombiniert Hybrid Search + Vector Search
  - `0` (default) â†’ Nur Hybrid Search (BM25 + Lexical)
  - `1` â†’ Hybrid Search + Vector Search kombiniert via RRF (beste Ergebnisse, aber langsamer)

**Business-Logic**:
- `VAT_RATE` - Mehrwertsteuer (default: 0.19)
- `ADOPT_THRESHOLD` - Threshold fÃ¼r Produkt-Adoption (default: 0.82)
- `LLM1_MODE` - `assistive` | `autonomous` (default: `assistive`)
- `BUSINESS_SCORING` - Komma-separierte Flags: `margin,availability`

**Development**:
- `DEBUG` - Debug-Modus (default: 0)
- `SKIP_LLM_SETUP` - LLM-Setup Ã¼berspringen (fÃ¼r Tests)
- `FORCE_RETRIEVER_BUILD` - Index immer neu bauen

**Security**:
- `ADMIN_API_KEY` - API-Key fÃ¼r Admin-Endpoints
- `FRONTEND_ORIGINS` - CORS-Origins (komma-separiert)

---

## ğŸ§ª Testing

**Test-Struktur**:
```
backend/testing/
â”œâ”€â”€ test_quote_service.py      # Service-Layer Tests
â”œâ”€â”€ test_retriever_thin.py      # Search-Tests
â”œâ”€â”€ test_store.py               # DB-Tests
â”œâ”€â”€ test_admin_api.py           # API-Tests
â””â”€â”€ test_smoke.py               # Smoke Tests
```

**Smoke Tests**:
- KÃ¶nnen ohne LLM laufen (`SKIP_LLM_SETUP=1`)
- PrÃ¼fen grundlegende FunktionalitÃ¤t
- Schnelle CI/CD-Integration

---

## ğŸš€ Deployment

**Docker**:
- `Dockerfile` vorhanden
- Port: 7860 (konfigurierbar via `PORT`)
- HuggingFace Spaces kompatibel

**Start**:
```bash
python main.py
# oder
uvicorn main:app --host 0.0.0.0 --port 7860
```

**Initialisierung**:
- DB wird automatisch erstellt beim ersten Start
- Index wird beim ersten Search gebaut
- Demo-User wird erstellt (`admin@kalkulai.de` / `kalkulai2024`)

---

## ğŸ“Š Performance-Optimierungen

1. **Caching**:
   - Catalog-Cache (60s TTL)
   - Search-Cache (pro Query + Top-K)
   - Index-Cache (pro Company)

2. **Pre-Filtering**:
   - Test-Produkte werden frÃ¼h ausgefiltert
   - Inactive Products werden ignoriert

3. **Hybrid Search**:
   - BM25 ist sehr schnell
   - Lexical Search ist deterministisch (keine LLM-Calls)
   - RRF kombiniert beide effizient

4. **Lazy Loading**:
   - Index wird erst beim ersten Search gebaut
   - Embeddings werden nur bei Bedarf generiert

---

## ğŸ” Wichtige Design-Entscheidungen

1. **Service Layer Pattern**:
   - Alle Business-Logik in `quote_service.py`
   - FastAPI-Endpoints sind dÃ¼nne Wrapper
   - MCP-Tools nutzen denselben Service-Layer

2. **Zwei-LLM-Architektur**:
   - LLM1: Kreativ, interaktiv (Chat)
   - LLM2: Deterministisch, strukturiert (Angebot)
   - Klare Trennung der Verantwortlichkeiten

3. **Hybrid Search**:
   - BM25 fÃ¼r Keyword-Matches
   - Lexical fÃ¼r Fuzzy-Matching
   - Keine AbhÃ¤ngigkeit von Vector-Embeddings fÃ¼r alle Suchen

4. **Multi-Tenant**:
   - Company-ID in allen DB-Queries
   - Separate Indizes pro Company
   - Isolation auf Datenbank-Ebene

5. **Stateful Sessions**:
   - Wizard-Sessions fÃ¼r mehrstufige Workflows
   - Memory wird zwischen Chat-Turns beibehalten
   - Reset-Endpoint fÃ¼r Session-Clearing

---

## ğŸ› Bekannte Limitationen & Tech Debt

1. **Legacy ChromaDB**:
   - Wird noch fÃ¼r statische Produktdateien verwendet
   - Sollte langfristig durch `index_manager` ersetzt werden

2. **Statische Produktdateien**:
   - `.txt`-Dateien in `data/` werden noch unterstÃ¼tzt
   - Migration zu DB-basiertem System ist im Gange

3. **Memory-Management**:
   - ConversationBufferWindowMemory begrenzt auf letzte N Nachrichten
   - Keine persistente Session-Storage

4. **Error-Handling**:
   - ServiceError wird verwendet, aber nicht Ã¼berall konsistent
   - Manche Fehler werden als HTTPException geworfen

---

## ğŸ“š WeiterfÃ¼hrende Dokumentation

- `docs/mcp-overview.md` - MCP-Architektur Details
- `PHASE1_IMPROVEMENTS.md` - Phase 1 Verbesserungen
- `PHASE2_MENGENBERECHNUNG.md` - Mengenberechnung-Logik
- `PHASE3_HYBRID_SEARCH.md` - Hybrid Search Implementation

---

## ğŸ’¡ Quick Start fÃ¼r neue Entwickler

1. **Repository klonen & Dependencies installieren**:
   ```bash
   cd backend
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Environment-Variablen setzen**:
   ```bash
   export OPENAI_API_KEY="sk-..."
   export MODEL_PROVIDER="openai"
   
   # Retriever-Konfiguration (optional):
   export LLM1_THIN_RETRIEVAL=0  # 0=Vector Index, 1=Hybrid Search fÃ¼r LLM1
   ```

3. **Backend starten**:
   ```bash
   python main.py
   ```

4. **Erste Schritte**:
   - `main.py` lesen â†’ Versteht App-Struktur
   - `app/services/quote_service.py` lesen â†’ Versteht Business-Logik
   - `retriever/thin.py` lesen â†’ Versteht Search-Logik
   - `store/catalog_store.py` lesen â†’ Versteht DB-Layer

5. **Tests ausfÃ¼hren**:
   ```bash
   pytest testing/
   ```

---

## ğŸ“ Code-Stil & Best Practices

- **Type Hints**: Ãœberall verwendet (`from __future__ import annotations`)
- **Docstrings**: Wichtige Funktionen haben Docstrings
- **Error-Handling**: ServiceError fÃ¼r strukturierte Fehler
- **Logging**: `logger.info()` fÃ¼r wichtige Events
- **Imports**: `from __future__ import annotations` am Anfang
- **Path-Handling**: `pathlib.Path` statt Strings

---

**Erstellt**: 2025-01-27  
**Version**: 1.0  
**Autor**: Backend-Team Kalkulai

